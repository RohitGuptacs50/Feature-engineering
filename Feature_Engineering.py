# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xqsqvVNDurUEt9agD1WAkqoAmfewbVGP
"""

!pip install -q xlrd

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/Concrete_Data.xls')
df.head()

"""encoding conversion(Optional)"""

import chardet
with open(file, 'rb') as rawdata:
  result = chardet.detect(rawdata.read(100000))
result

df = pd.read_csv(file,encoding= "latin-1")
df.head()

X = df.copy()
y = X.pop('CompressivrStrength')

# Train and score baseline model
baseline = RandomForestRegressor(criterion='mae', random_state=0)
baseline_score = cross_val_score(baseline, X, y, cv = 5, scoring='neg_mean_absolute_error')

baseline_score = -1 * baseline_score.mean()

print(f'MAE Baseline Score: {baseline_score:.4}')

X = df.copy()
y = X.pop('CompressivrStrength')

X['FCRatio'] = X["FineAggregate "] / X['CoarseAggregate  ']
X['AggCmtRatio'] = (X['CoarseAggregate  '] + X['FineAggregate ']) / X['Cement']
X['WtrCmtRatio'] = X['Water  '] / X['Cement']

model = RandomForestRegressor(criterion='mae', random_state=0)
score = cross_val_score(model, X, y, cv = 5, scoring='neg_mean_absolute_error')
score = -1 * score.mean()
print(f'MAE Score with Ratio Features: {score: .4}')







import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use('seaborn-whitegrid')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/Automobile_data.csv')
df.head()

list_of_column_names = list(df.columns)
print(list_of_column_names)



df.drop('normalized-losses', axis = 1, inplace = True)
df.head()

df.replace('?', np.NaN, inplace=True)
df.head(11)

df.fillna(0, inplace=True)
df.head(11)

X = df.copy()
y = X.pop('price')



# Label encoding for categoricals
for colname in X.select_dtypes('object'):
  X[colname], _ = X[colname].factorize()


# All discrete features should now have integer dtypes (double-check this before using MI!)
discrete_features = X.dtypes == int

from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
  mi_scores = mutual_info_regression(X, y, discrete_features = discrete_features)
  mi_scores = pd.Series(mi_scores, name = 'MI SCORES', index = X.columns)
  mi_scores = mi_scores.sort_values(ascending = False)
  return mi_scores 

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]

def plot_mi_scores(scores):
  scores = scores.sort_values(ascending = True)
  width = np.arange(len(scores))
  ticks = list(scores.index)
  plt.barh(width, scores)
  plt.yticks(width, ticks)
  plt.title('Mutual Information Scores')

plt.figure(dpi = 100, figsize = (8, 5))
plot_mi_scores(mi_scores)

weight = df['curb-weight'].astype(int) 
price = df['price'].astype(int)

sns.relplot(x=weight, y=price, data=df)

df.dtypes

df['fuel-type'].dtype

horsepower = df['horsepower'].astype(str).astype(int)
price = df['price'].astype(str).astype(int)
print(horsepower.dtype)
print(price.dtype)



sns.relplot(x=horsepower, y=price, data=df)









import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use('seaborn-whitegrid')
plt.rc('figure', autolayout = True)
plt.rc(
    'axes',
    labelweight = 'bold',
    labelsize = 'large',
    titleweight = 'bold',
    titlesize = 14,
    titlepad = 10,
)

accidents = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/US_Accidents_Dec20.csv')
autos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/Automobile_data.csv')
concrete = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/Concrete_Data.xls')
customer = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')

autos.head()

autos.dtypes

autos.replace('?', np.NaN, inplace=True)
autos.fillna(0, inplace=True)

stroke = autos['stroke'].astype(str).astype(float)
bore = autos['bore'].astype(str).astype(float)

stroke = autos['stroke'].astype(str).astype(float)
bore = autos['bore'].astype(str).astype(float)
autos['stroke_ratio'] = stroke / bore

autos[['stroke', 'bore', 'stroke_ratio']].head()

stroke = autos['stroke'].astype(str).astype(float)

autos['num-of-cylinders'].astype(str)

dicti = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'twelve': 12}
autos.replace({'num-of-cylinders': dicti}, inplace = True)

autos.head(10)

autos['num-of-cylinders'].dtype

num_of_cyl = autos['num-of-cylinders']

autos['displacement'] = (
    np.pi * ((0.5 * bore) ** 2) * stroke * num_of_cyl
)

autos['displacement'].head()





accidents.head()

accidents.dtypes

accidents.fillna(0, inplace=True)

accidents.head()

accidents['LogWindSpeed'] = accidents['Wind_Speed(mph)'].apply(np.log1p)

fig, axs = plt.subplots(1, 2, figsize = (8, 4))
sns.kdeplot(accidents['Wind_Speed(mph)'], shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade = True, ax=axs[1])

roadway_features = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 
                    'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',
                    'Traffic_Calming', 'Traffic_Signal']

accidents['RoadwayFeatures'] = accidents[roadway_features].sum(axis = 1)
accidents[roadway_features + ['RoadwayFeatures']].head(10)

concrete.dtypes

components = ['Cement', 'BlastFurnaceSlag ', 'FlyAsh ', 'Water  ',
              'Superplasticizer ', 'CoarseAggregate  ', 'FineAggregate ']
concrete['Components'] = concrete[components].gt(0).sum(axis = 1)
concrete[components + ['Components']].head(10)

customer.dtypes

customer.head()

customer[['Type', 'Level']] = (customer['Policy'].str.split(' ', expand = True))

customer[['Policy', 'Type', 'Level']].head(10)

autos['make'].astype(str)
autos['body-style'].astype(str)

print(autos['make'].dtype)
print(autos['body-style'].dtype)

autos["make_and_style"] = autos["make"] + "_" + autos["body-style"]
autos[['make', 'body-style', 'make_and_style']].head()

customer['AverageIncome'] = (
    customer.groupby('State')['Income'].transform('mean')
)

customer[["State", "Income", "AverageIncome"]].head(10)

customer['StateFreq'] = (customer.groupby('State')['State'].transform('count')/customer.State.count())

customer[["State", "StateFreq"]].head(10)

df_train = customer.sample(frac = 0.5)
df_valid = customer.drop(df_train.index)

df_train['AverageClaim'] = df_train.groupby('Coverage')['Total Claim Amount'].transform('mean')


df_valid = df_valid.merge(df_train[['Coverage', 'AverageClaim']].drop_duplicates(), on = 'Coverage', how = 'left')

df_valid[['Coverage', 'AverageClaim']].head(10)









import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans


plt.style.use('seaborn-whitegrid')
plt.rc('figure', autolayout = True)
plt.rc(
    'axes',
    labelweight = 'bold',
    labelsize = 'large',
    titleweight = 'bold',
    titlesize = 14,
    titlepad = 10,
)

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/housing.csv')
df.head()

X = df.loc[:, ['median_income', 'latitude', 'longitude']]
X.head()

kmeans = KMeans(n_clusters=6)
X['Cluster'] = kmeans.fit_predict(X)
X['Cluster'] = X['Cluster'].astype('category')

X.head()

sns.relplot(x = 'longitude', y = 'latitude', hue = 'Cluster', data = X, height = 7)

X['MedHouseVal'] = df['median_house_value']
sns.catplot(x = 'MedHouseVal', y = 'Cluster', data = X, kind = 'boxen', height = 7)







from IPython.display import display
from sklearn.feature_selection import mutual_info_regression

plt.style.use('seaborn-whitegrid')
plt.rc('figure', autolayout = True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)


def plot_variance(pca, width = 0, dpi = 100):
  fig, axs = plt.subplots(1, 2)
  n = pca.n_components_
  grid = np.arange(1, n + 1)

  evr = pca.explained_variance_ratio_
  axs[0].bar(grid, evr)
  axs[0].set(
      xlabel = 'Component', title = '% Explained Variance', ylim = (0.0, 1.0)
  )

  cv = np.cumsum(evr)
  axs[1].plot(np.r_[0, grid], np.r_[0, cv], 'o-')
  axs[1].set(xlabel = 'Component', title = '% Cumulative Variance', ylim = (0.0, 1.0))

  fig.set(figwidth = 8, dpi = 100)
  return axs


def make_mi_scores(X, y, discrete_features):
  mi_scores = mutual_info_regression(X, y, discrete_features = discrete_features)
  mi_scores = pd.Series(mi_scores, name = 'MI Scores', index = X.columns)
  mi_scores = mi_scores.sort_values(ascending = False)
  return mi_scores



df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Feature engineering Kaggle/Automobile_data.csv')

df.dtypes



features = ['highway-mpg', 'engine-size', 'horsepower', 'curb-weight']

X = df.copy()
y = X.pop('price')
X = X.loc[:, features]

X_scaled = (X - X.mean(axis = 0)) / X.std(axis = 0)

from sklearn.decomposition import PCA

pca = PCA()
X_pca = pca.fit_transform(X_scaled)

component_names = [f'PC{i + 1}' for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns = component_names)

X_pca.head()

loadings = pd.DataFrame(
    pca.components_.T,    # transpose the matrix of loadings 
    columns = component_names,   # so the columns are the principal components
    index = X.columns,   # and the rows are the original features
)

loadings

# variance
plot_variance(pca)

mi_scores = make_mi_scores(X_pca, y, discrete_features=False)
mi_scores

# Show dataframe sorted by PC3
idx = X_pca['PC3'].sort_values(ascending = False).index
cols = ['make', 'body-style', 'horsepower', 'curb-weight']
df.loc[idx, cols]

df['sports_or_wagon'] = X['curb-weight'] / X['horsepower']
sns.regplot(x = 'sports_or_wagon', y = 'price', data = df, order = 2)











autos['make_encoded'] = autos.groupby('make')['price'].transform('mean')

autos[['make', 'price', 'make_encoded']].head(10)







